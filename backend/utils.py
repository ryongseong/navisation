# 1. PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
from pdfminer.high_level import extract_text
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import json
import time
from tqdm import tqdm
from langchain_upstage import ChatUpstage
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain_upstage import UpstageEmbeddings
from ragas.metrics import context_precision, context_recall
from ragas import evaluate
from datasets import Dataset

UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")


def parse_text_from_pdf(pdf_path):
    # pdf_path = "stay.pdf"  # PDF ê²½ë¡œ
    pdf_text = extract_text(pdf_path)
    print(pdf_text[:500])  # ì¼ë¶€ ë¯¸ë¦¬ë³´ê¸°
    return pdf_text


# 2. í…ìŠ¤íŠ¸ â†’ ì²­í¬ ë¶„í• 
def chunk_text(text, chunk_size=1000, chunk_overlap=200):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.create_documents([text])


# 3. ëª¨ë¸ ë° íŒŒì„œ ì„¤ì •
def add_topic_on_each_chunk():
    llm = ChatUpstage(UPSTAGE_API_KEY)
    output_parser = StrOutputParser()

    # ì£¼ì œ ë¦¬ìŠ¤íŠ¸
    # (ì‚¬ì¦ ë§¤ë‰´ì–¼)
    # visa_topics = [
    #     "ê¸°íƒ€", "ì™¸êµ(A-1)", "ê³µë¬´(A-2)", "í˜‘ì •(A-3)", "ì‚¬ì¦ë©´ì œ(B-1)", "ê´€ê´‘í†µê³¼(B-2)",
    #     "ì¼ì‹œì·¨ì¬(C-1)", "ë‹¨ê¸°ë°©ë¬¸(C-3)", "ë‹¨ê¸°ì·¨ì—…(C-4)", "ë¬¸í™”ì˜ˆìˆ (D-1)", "ìœ í•™(D-2)",
    #     "ê¸°ìˆ ì—°ìˆ˜(D-3)", "ì¼ë°˜ì—°ìˆ˜(D-4)", "ì·¨ì¬(D-5)", "ì¢…êµ(D-6)", "ì£¼ì¬(D-7)",
    #     "ê¸°ì—…íˆ¬ì(D-8)", "ë¬´ì—­ê²½ì˜(D-9)", "êµ¬ì§(D-10)", "êµìˆ˜(E-1)", "íšŒí™”ì§€ë„(E-2)",
    #     "ì—°êµ¬(E-3)", "ê¸°ìˆ ì§€ë„(E-4)", "ì „ë¬¸ì§ì—…(E-5)", "ì˜ˆìˆ í¥í–‰(E-6)", "íŠ¹ì •í™œë™(E-7)",
    #     "ê³„ì ˆê·¼ë¡œ(E-8)", "ë¹„ì „ë¬¸ì·¨ì—…(E-9)", "ì„ ì›ì·¨ì—…(E-10)", "ë°©ë¬¸ë™ê±°(F-1)", "ê±°ì£¼(F-2)",
    #     "ë™ë°˜(F-3)", "ì¬ì™¸ë™í¬(F-4)", "ì˜ì£¼(F-5)", "ê²°í˜¼ì´ë¯¼(F-6)", "ê¸°íƒ€(G-1)",
    #     "ê´€ê´‘ì·¨ì—…(H-1)", "ë°©ë¬¸ì·¨ì—…(H-2)", "íƒ‘í‹°ì–´ë¹„ì(D-10-T)", "íƒ‘í‹°ì–´ë¹„ì(E-7-T)",
    #     "íƒ‘í‹°ì–´ë¹„ì(F-2-T)", "íƒ‘í‹°ì–´ë¹„ì(F-5-T)"
    # ]

    # ì£¼ì œ ë¦¬ìŠ¤íŠ¸ (ì²´ë¥˜ ë§¤ë‰´ì–¼)
    visa_topics = [
        "ê¸°íƒ€", "ì™¸êµ(A-1)", "ê³µë¬´(A-2)", "í˜‘ì •(A-3)", "ì‚¬ì¦ë©´ì œ(B-1)", "ê´€ê´‘í†µê³¼(B-2)",
        "ì¼ì‹œì·¨ì¬(C-1)", "ë‹¨ê¸°ë°©ë¬¸(C-3)", "ë‹¨ê¸°ì·¨ì—…(C-4)", "ë¬¸í™”ì˜ˆìˆ (D-1)", "ìœ í•™(D-2)",
        "ê¸°ìˆ ì—°ìˆ˜(D-3)", "ì¼ë°˜ì—°ìˆ˜(D-4)", "ì·¨ì¬(D-5)", "ì¢…êµ(D-6)", "ì£¼ì¬(D-7)",
        "ê¸°ì—…íˆ¬ì(D-8)", "ë¬´ì—­ê²½ì˜(D-9)", "êµ¬ì§(D-10)", "êµìˆ˜(E-1)", "íšŒí™”ì§€ë„(E-2)",
        "ì—°êµ¬(E-3)", "ê¸°ìˆ ì§€ë„(E-4)", "ì „ë¬¸ì§ì—…(E-5)", "ì˜ˆìˆ í¥í–‰(E-6)", "íŠ¹ì •í™œë™(E-7)",
        "ê³„ì ˆê·¼ë¡œ(E-8)", "ë¹„ì „ë¬¸ì·¨ì—…(E-9)", "ì„ ì›ì·¨ì—…(E-10)", "ë°©ë¬¸ë™ê±°(F-1)", "ê±°ì£¼(F-2)",
        "ë™ë°˜(F-3)", "ì¬ì™¸ë™í¬(F-4)", "ì˜ì£¼(F-5)", "ê²°í˜¼ì´ë¯¼(F-6)", "ê¸°íƒ€(G-1)",
        "ê´€ê´‘ì·¨ì—…(H-1)", "ë°©ë¬¸ì·¨ì—…(H-2)", "êµ­ë‚´ ì„±ì¥ ê¸°ë°˜ ì™¸êµ­ì¸ ì²­ì†Œë…„ ì·¨ì—….ì •ì£¼ ì²´ë¥˜ì œë„", "íƒ‘í‹°ì–´ë¹„ì(D-10-T)", "íƒ‘í‹°ì–´ë¹„ì(E-7-T)",
        "íƒ‘í‹°ì–´ë¹„ì(F-2-T)", "íƒ‘í‹°ì–´ë¹„ì(F-5-T)"
    ]

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_template = PromptTemplate.from_template("""
    ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì£¼ì œë¥¼ ë°˜ë“œì‹œ ì•„ë˜ ë¦¬ìŠ¤íŠ¸ ì¤‘ í•˜ë‚˜ë§Œ ê³¨ë¼ì£¼ì„¸ìš”.
    ì ˆëŒ€ ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ë‹¨ì–´ë“¤ì„ ì‚¬ìš©í•´ì„œ ì£¼ì œë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”. ë‚˜ì¤‘ì— ì£¼ì œëª…ìœ¼ë¡œ ë¶„ë¥˜í•  ì˜ˆì •ì´ë¼ í•­ìƒ ê°™ì€ ë‹¨ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    
    í…ìŠ¤íŠ¸:
    \"\"\"{text}\"\"\"
    
    ì£¼ì œ ë¦¬ìŠ¤íŠ¸:
    {topics}
    
    ì£¼ì œ:
    """)
    chain = prompt_template | llm | output_parser

    # ì¤‘ê°„ ì €ì¥ìš© íŒŒì¼
    SAVE_PATH = "chunk_with_topic_stay.jsonl"

    # ê¸°ì¡´ ì €ì¥ëœ ì²­í¬ ë¶ˆëŸ¬ì˜¤ê¸° (ìˆìœ¼ë©´ ì´ì–´ì„œ)
    existing_chunks = set()
    if os.path.exists(SAVE_PATH):
        with open(SAVE_PATH, "r", encoding="utf-8") as f:
            for line in f:
                data = json.loads(line)
                existing_chunks.add(data["content"][:100])  # ì¤‘ë³µ ì²´í¬ìš© (100ì ë¯¸ë¦¬ë³´ê¸°)

    # ì§„í–‰
    with open(SAVE_PATH, "a", encoding="utf-8") as out_file:
        for i, doc in enumerate(tqdm(docs, desc="í† í”½ ë¶„ë¥˜ ì¤‘")):
            preview = doc.page_content[:100]

            if preview in existing_chunks:
                continue  # ì´ë¯¸ ì²˜ë¦¬ëœ ì²­í¬ëŠ” ìŠ¤í‚µ

            # API í˜¸ì¶œ
            try:
                topic = chain.invoke({
                    "text": doc.page_content,
                    "topics": ", ".join(visa_topics)
                }).strip()
            except Exception as e:
                print(f"[ì—ëŸ¬] ì²­í¬ {i + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                time.sleep(5)
                continue

            doc.metadata["topic"] = topic

            # ì¶œë ¥
            print(f"\n[ì²­í¬ {i + 1}]")
            print(f"í† í”½: {topic}")
            print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{preview}...")
            print("-" * 40)

            # ì €ì¥
            save_obj = {
                "content": doc.page_content,
                "metadata": doc.metadata
            }
            out_file.write(json.dumps(save_obj, ensure_ascii=False) + "\n")
            out_file.flush()

            # ì¿¼í„° ë³´í˜¸
            if i % 5 == 0:
                time.sleep(2)


# ì²˜ìŒ ë°ì´í„° ë²¡í„°ìŠ¤í† ì–´ ì €ì¥í•˜ëŠ” ì½”ë“œ
def save_as_vectorstore():
    # 1. jsonl íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    jsonl_path = "chunk_with_topic_stay_output.jsonl"
    documents = []

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            content = item["content"]
            metadata = item["metadata"]
            documents.append(Document(page_content=content, metadata=metadata))

    # 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    embedding_model = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model="solar-embedding-1-large")

    # 3. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(documents, embedding_model)

    # 4. ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ (ë¡œì»¬)
    faiss_save_path = "faiss_store"
    vectorstore.save_local(faiss_save_path)

    print(f"âœ… {len(documents)}ê°œì˜ ì²­í¬ê°€ FAISSì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")



# ì²« ë²ˆì§¸ ì´í›„ ë°ì´í„° ë²¡í„°ìŠ¤í† ì–´ ì €ì¥í•˜ëŠ” ì½”ë“œ
def save_next_one_as_vectorstore():
    # =============================================================================
    # 1. ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    # =============================================================================

    print("ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")

    try:
        existing_vectorstore = FAISS.load_local(
            "faiss_store",
            UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model="solar-embedding-1-large"),
            allow_dangerous_deserialization=True
        )
        print(f"âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ: {len(existing_vectorstore.index_to_docstore_id)}ê°œ ë¬¸ì„œ")
    except Exception as e:
        print(f"âŒ ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ìƒˆë¡œìš´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        existing_vectorstore = None

    # =============================================================================
    # 2. ìƒˆë¡œìš´ JSONL íŒŒì¼ ë¡œë“œ â†’ Document ê°ì²´ë¡œ ë³€í™˜
    # =============================================================================

    def load_jsonl_to_documents(jsonl_path):
        """JSONL íŒŒì¼ì„ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        documents = []

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(jsonl_path):
            raise FileNotFoundError(f"JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")

        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                try:
                    item = json.loads(line.strip())

                    # content í•„ë“œ í™•ì¸
                    if "content" not in item or not item["content"].strip():
                        print(f"âš ï¸  ë¼ì¸ {line_num}: contentê°€ ë¹„ì–´ìˆìŒ - ê±´ë„ˆëœ€")
                        continue

                    content = item["content"]
                    metadata = item.get("metadata", {})
                    documents.append(Document(page_content=content, metadata=metadata))

                except json.JSONDecodeError as e:
                    print(f"âš ï¸  ë¼ì¸ {line_num}: JSON íŒŒì‹± ì—ëŸ¬ - {e}")
                    continue

        return documents

    # ìƒˆ ë¬¸ì„œ ë¡œë“œ
    print("\nìƒˆë¡œìš´ JSONL íŒŒì¼ ë¡œë“œ ì¤‘...")
    try:
        new_docs = load_jsonl_to_documents("chunk_with_topic_stay_output.jsonl")
        print(f"âœ… ìƒˆ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(new_docs)}ê°œ ë¬¸ì„œ")
    except Exception as e:
        print(f"âŒ JSONL ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

    # =============================================================================
    # 3. ì›ë˜ ë°©ì‹ëŒ€ë¡œ ë¯¸ë¦¬ ì„ë² ë”© ìƒì„± í›„ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€ (API íš¨ìœ¨ì„±)
    # =============================================================================

    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embedding_model = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model="solar-embedding-1-large")

    # ë°°ì¹˜ ì„¤ì •
    BATCH_SIZE = 50  # API ì œí•œì— ë”°ë¼ ì¡°ì • (30-100 ì‚¬ì´ ê¶Œì¥)
    DELAY_SECONDS = 1.0  # ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)

    print(f"\në°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(new_docs)}ê°œ ë¬¸ì„œë¥¼ {BATCH_SIZE}ê°œì”© ì²˜ë¦¬")
    print(f"ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„: {DELAY_SECONDS}ì´ˆ")

    total_batches = (len(new_docs) + BATCH_SIZE - 1) // BATCH_SIZE
    current_vectorstore = existing_vectorstore

    for i in range(0, len(new_docs), BATCH_SIZE):
        batch_docs = new_docs[i:i + BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1

        print(f"\nğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_docs)}ê°œ ë¬¸ì„œ)")

        try:
            # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            texts = [doc.page_content for doc in batch_docs]
            metadatas = [doc.metadata for doc in batch_docs]

            # ì„ë² ë”© ìƒì„± (API í˜¸ì¶œ - í•œ ë²ˆë§Œ)
            print("   ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = embedding_model.embed_documents(texts)
            print(f"   ğŸ“Š ìƒì„±ëœ ì„ë² ë”© ì •ë³´: {len(embeddings)}ê°œ, ì°¨ì›: {len(embeddings[0]) if embeddings else 0}")

            # ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€ (ë””ë²„ê¹… í¬í•¨)
            if current_vectorstore is None:
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë©´ ìƒˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                print("   ğŸ†• ìƒˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±")
                current_vectorstore = FAISS.from_texts(
                    texts=texts,
                    embedding=embedding_model,
                    metadatas=metadatas
                )
            else:
                # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€ - ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„
                print("   â• ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€")

                try:
                    # ë°©ë²• 1: ì›ë˜ ì½”ë“œ ë°©ì‹
                    print("     ğŸ”§ ë°©ë²• 1: add_embeddings ì‹œë„...")
                    current_vectorstore.add_embeddings(
                        texts=texts,
                        text_embeddings=embeddings,
                        metadatas=metadatas
                    )
                    print("     âœ… add_embeddings ì„±ê³µ!")

                except Exception as e1:
                    print(f"     âŒ add_embeddings ì‹¤íŒ¨: {e1}")

                    try:
                        # ë°©ë²• 2: add_texts ì‚¬ìš©
                        print("     ğŸ”§ ë°©ë²• 2: add_texts ì‹œë„...")
                        current_vectorstore.add_texts(
                            texts=texts,
                            metadatas=metadatas
                        )
                        print("     âœ… add_texts ì„±ê³µ!")

                    except Exception as e2:
                        print(f"     âŒ add_textsë„ ì‹¤íŒ¨: {e2}")
                        raise e2

            print(f"   âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ")

            # API ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸° (ë§ˆì§€ë§‰ ë°°ì¹˜ ì œì™¸)
            if i + BATCH_SIZE < len(new_docs):
                print(f"   â³ {DELAY_SECONDS}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(DELAY_SECONDS)

        except Exception as e:
            print(f"   âŒ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
            print("   â­ï¸  ë‹¤ìŒ ë°°ì¹˜ë¡œ ê³„ì† ì§„í–‰...")
            continue

    # ê²°ê³¼ í™•ì¸
    if current_vectorstore:
        total_docs = len(current_vectorstore.index_to_docstore_id)
        print(f"\nğŸ‰ ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")
    else:
        print("\nâŒ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨")

    # =============================================================================
    # 4. ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
    # =============================================================================

    if current_vectorstore:
        print("\nğŸ’¾ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì¤‘...")
        try:
            current_vectorstore.save_local("faiss_store")
            print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: faiss_store")
        except Exception as e:
            print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨: {e}")
    else:
        print("âŒ ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("\nğŸš€ ì‘ì—… ì™„ë£Œ!")

    # =============================================================================
    # 5. ê²°ê³¼ í™•ì¸ (ì„ íƒì‚¬í•­)
    # =============================================================================

    # ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ í™•ì¸
    print("\nğŸ“‹ ìµœì¢… ê²°ê³¼ í™•ì¸...")
    try:
        final_vectorstore = FAISS.load_local(
            "faiss_store",
            UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model="solar-embedding-1-large"),
            allow_dangerous_deserialization=True
        )
        print(f"âœ… ìµœì¢… ë²¡í„°ìŠ¤í† ì–´ ë¬¸ì„œ ìˆ˜: {len(final_vectorstore.index_to_docstore_id)}ê°œ")
    except Exception as e:
        print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ í™•ì¸ ì‹¤íŒ¨: {e}")


def load_vectorstore(UPSTAGE_API_KEY):
    # 1. ê°™ì€ ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë‹¤ì‹œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤ (ì´ê²Œ ì¤‘ìš”!)
    embedding_model = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model="solar-embedding-1-large")

    # 2. FAISS ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ì§€ì • (ì˜ˆ: 'faiss_store')
    vectorstore = FAISS.load_local("faiss_store", embedding_model, allow_dangerous_deserialization=True)
    return vectorstore


def get_answer(vectorstore, question, lang):
    llm = ChatUpstage()
    embedding_model = UpstageEmbeddings(model="embedding-query")
    language = lang

    visa_topics = [
        "ê¸°íƒ€", "ì™¸êµ(A-1)", "ê³µë¬´(A-2)", "í˜‘ì •(A-3)", "ì‚¬ì¦ë©´ì œ(B-1)", "ê´€ê´‘í†µê³¼(B-2)",
        "ì¼ì‹œì·¨ì¬(C-1)", "ë‹¨ê¸°ë°©ë¬¸(C-3)", "ë‹¨ê¸°ì·¨ì—…(C-4)", "ë¬¸í™”ì˜ˆìˆ (D-1)", "ìœ í•™(D-2)",
        "ê¸°ìˆ ì—°ìˆ˜(D-3)", "ì¼ë°˜ì—°ìˆ˜(D-4)", "ì·¨ì¬(D-5)", "ì¢…êµ(D-6)", "ì£¼ì¬(D-7)",
        "ê¸°ì—…íˆ¬ì(D-8)", "ë¬´ì—­ê²½ì˜(D-9)", "êµ¬ì§(D-10)", "êµìˆ˜(E-1)", "íšŒí™”ì§€ë„(E-2)",
        "ì—°êµ¬(E-3)", "ê¸°ìˆ ì§€ë„(E-4)", "ì „ë¬¸ì§ì—…(E-5)", "ì˜ˆìˆ í¥í–‰(E-6)", "íŠ¹ì •í™œë™(E-7)",
        "ê³„ì ˆê·¼ë¡œ(E-8)", "ë¹„ì „ë¬¸ì·¨ì—…(E-9)", "ì„ ì›ì·¨ì—…(E-10)", "ë°©ë¬¸ë™ê±°(F-1)", "ê±°ì£¼(F-2)",
        "ë™ë°˜(F-3)", "ì¬ì™¸ë™í¬(F-4)", "ì˜ì£¼(F-5)", "ê²°í˜¼ì´ë¯¼(F-6)", "ê¸°íƒ€(G-1)",
        "ê´€ê´‘ì·¨ì—…(H-1)", "ë°©ë¬¸ì·¨ì—…(H-2)", "íƒ‘í‹°ì–´ë¹„ì(D-10-T)", "íƒ‘í‹°ì–´ë¹„ì(E-7-T)",
        "íƒ‘í‹°ì–´ë¹„ì(F-2-T)", "íƒ‘í‹°ì–´ë¹„ì(F-5-T)"
    ]

    # ì£¼ì œ ë¦¬ìŠ¤íŠ¸ (ì²´ë¥˜ ë§¤ë‰´ì–¼)
    stay_topics = [
        "ê¸°íƒ€", "ì™¸êµ(A-1)", "ê³µë¬´(A-2)", "í˜‘ì •(A-3)", "ì‚¬ì¦ë©´ì œ(B-1)", "ê´€ê´‘í†µê³¼(B-2)",
        "ì¼ì‹œì·¨ì¬(C-1)", "ë‹¨ê¸°ë°©ë¬¸(C-3)", "ë‹¨ê¸°ì·¨ì—…(C-4)", "ë¬¸í™”ì˜ˆìˆ (D-1)", "ìœ í•™(D-2)",
        "ê¸°ìˆ ì—°ìˆ˜(D-3)", "ì¼ë°˜ì—°ìˆ˜(D-4)", "ì·¨ì¬(D-5)", "ì¢…êµ(D-6)", "ì£¼ì¬(D-7)",
        "ê¸°ì—…íˆ¬ì(D-8)", "ë¬´ì—­ê²½ì˜(D-9)", "êµ¬ì§(D-10)", "êµìˆ˜(E-1)", "íšŒí™”ì§€ë„(E-2)",
        "ì—°êµ¬(E-3)", "ê¸°ìˆ ì§€ë„(E-4)", "ì „ë¬¸ì§ì—…(E-5)", "ì˜ˆìˆ í¥í–‰(E-6)", "íŠ¹ì •í™œë™(E-7)",
        "ê³„ì ˆê·¼ë¡œ(E-8)", "ë¹„ì „ë¬¸ì·¨ì—…(E-9)", "ì„ ì›ì·¨ì—…(E-10)", "ë°©ë¬¸ë™ê±°(F-1)", "ê±°ì£¼(F-2)",
        "ë™ë°˜(F-3)", "ì¬ì™¸ë™í¬(F-4)", "ì˜ì£¼(F-5)", "ê²°í˜¼ì´ë¯¼(F-6)", "ê¸°íƒ€(G-1)",
        "ê´€ê´‘ì·¨ì—…(H-1)", "ë°©ë¬¸ì·¨ì—…(H-2)", "êµ­ë‚´ ì„±ì¥ ê¸°ë°˜ ì™¸êµ­ì¸ ì²­ì†Œë…„ ì·¨ì—….ì •ì£¼ ì²´ë¥˜ì œë„", "íƒ‘í‹°ì–´ë¹„ì(D-10-T)", "íƒ‘í‹°ì–´ë¹„ì(E-7-T)",
        "íƒ‘í‹°ì–´ë¹„ì(F-2-T)", "íƒ‘í‹°ì–´ë¹„ì(F-5-T)"
    ]

    # 2. í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompt_template = PromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì¶œì…êµ­ ê´€ë ¨ ë§¤ë‰´ì–¼ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì´ì í†µì—­ê°€ì…ë‹ˆë‹¤. ì œê³µë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒ ë‘ ì¢…ë¥˜ì˜ ë§¤ë‰´ì–¼ ì¤‘ í•˜ë‚˜ ë˜ëŠ” ëª¨ë‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

        1. ì‚¬ì¦ ë§¤ë‰´ì–¼ (Visa Manual) â€“ í•œêµ­ ì…êµ­ ì „ ë¹„ì ë°œê¸‰ì— ëŒ€í•œ ì •ë³´  
        2. ì²´ë¥˜ ë§¤ë‰´ì–¼ (Stay Manual) â€“ ì…êµ­ í›„ ì²´ë¥˜ ì—°ì¥, ì²´ë¥˜ ìê²© ë³€ê²½ ë“±ì— ëŒ€í•œ ì •ë³´

        ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì˜ ì½ê³ , ì•„ë˜ ì§€ì¹¨ì— ë”°ë¼ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ì–¸ì–´ì¸ {language}ìœ¼ë¡œ ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ í•´ ì£¼ì„¸ìš”:
        - ë°˜ë“œì‹œ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ì–¸ì–´ì¸ {language}ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”!!!
        - ê° ë¹„ìì—ëŠ” ì„¸ë¶€ ìœ í˜•(sub-type)ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: D-8-1, D-8-4).  
          â†’ ê° ì„¸ë¶€ ë¹„ìë³„ ìš”ê±´ ë° ì œì¶œ ì„œë¥˜ê°€ ë‹¤ë¥´ë¯€ë¡œ, ë°˜ë“œì‹œ ì •í™•í•œ ì„¸ë¶€ ìœ í˜•ì„ êµ¬ë¶„í•˜ì—¬ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ì–¸ì–´ì¸ {language}ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        - ê° ë¹„ìì— ëŒ€í•œ ì œì¶œ ì„œë¥˜, ëŒ€ìƒì, ìê²©ìš”ê±´ ë“±ì€ ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•˜ëŠ”ì§€, ì•„ë‹ˆë©´ ì¼ë¶€ë§Œ ë§Œì¡±í•´ë„ ë˜ëŠ”ì§€ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        - ì ìˆ˜ì œì—ë„ ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. ë¹„ìì— ë”°ë¼ í•´ë‹¹í•˜ëŠ” ì ìˆ˜ì œê°€ ë‹¬ë¼ì§€ë‹ˆ ìœ ì˜í•´ì„œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        - ì»¨í…ìŠ¤íŠ¸ì— ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì£¼ì–´ì§„ ë‚´ìš©ì— ê¸°ë°˜í•´ì„œë§Œ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ì–¸ì–´ì¸ {language}ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

        ---

        ì§ˆë¬¸:
        {question}

        ---

        ì»¨í…ìŠ¤íŠ¸:
        {context}

        ---

        ì‚¬ì¦ ì£¼ì œ ë¦¬ìŠ¤íŠ¸:
        {visa_topics}

        ì²´ë¥˜ ì£¼ì œ ë¦¬ìŠ¤íŠ¸:
        {stay_topics}

        ë‹µë³€:
        """)

    chain = prompt_template | llm | StrOutputParser()

    # 3. Retriever ì •ì˜
    faiss_retriever = vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={"k": 20}  # ì›ë³¸ë³´ë‹¤ ì¡°ê¸ˆ ë” ë„“ê²Œ ê²€ìƒ‰
    )

    topic_chain = PromptTemplate.from_template("""
                         ë‹¤ìŒ ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ì£¼ì œë¥¼ ì•„ë˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•˜ë‚˜ ì„ íƒí•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë§ë¶™ì´ëŠ” ë§ ì—†ì´ ì£¼ì œ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”:

                         ì§ˆë¬¸: {question}
                         ì£¼ì œ ë¦¬ìŠ¤íŠ¸: {all_topics}

                         ì„ íƒí•œ ì£¼ì œ:""") | llm | StrOutputParser()

    # all_topicsëŠ” visa_topics + stay_topics
    all_topics = visa_topics + stay_topics

    # 1. í† í”½ ì¶”ë¡ 
    try:
        inferred_topic = topic_chain.invoke({
            "question": question,
            "all_topics": ", ".join(visa_topics + stay_topics)
        }).strip()
        print(f"{inferred_topic}")
    except Exception as e:
        print(f"âŒ í† í”½ ì¶”ë¡  ì‹¤íŒ¨: '{question}' / ì—ëŸ¬: {e}")
        inferred_topic = ""

    # 2. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    all_results = faiss_retriever.invoke(question)

    # 3. í† í”½ ê¸°ë°˜ í•„í„°ë§ (ì •ìƒ ì¶”ë¡ ëœ ê²½ìš°ë§Œ)
    if inferred_topic:
        filtered_docs = [doc for doc in all_results if inferred_topic in doc.metadata.get("topic", "")]
    else:
        filtered_docs = []

    # 4. ë¬¸ì„œ ì„ íƒ ì „ëµ
    if not inferred_topic:
        print(f"âš ï¸ [í† í”½ ì¶”ë¡  ì‹¤íŒ¨] '{question}' â†’ ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
        context_docs = all_results
    elif not filtered_docs:
        print(f"âš ï¸ [í† í”½ ë¶ˆì¼ì¹˜] '{question}' â†’ ì¶”ë¡ ëœ í† í”½: {inferred_topic}, í•„í„°ë§ëœ ë¬¸ì„œ ì—†ìŒ â†’ ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
        context_docs = all_results
    else:
        print(f"ğŸ” ì¶”ë¡ ëœ í† í”½: {inferred_topic}")
        context_docs = filtered_docs

    # 5. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = [doc.page_content for doc in context_docs]
    context_str = "\n\n".join(context)

    # 6. LLM í˜¸ì¶œ (í•„ìˆ˜ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ í¬í•¨)
    answer = chain.invoke({
        "context": context_str,
        "question": question,
        "language": language,
        "visa_topics": ", ".join(visa_topics),
        "stay_topics": ", ".join(stay_topics)
    })

    return answer


# def get_answer_and_evaluate(vectorstore):
#     # 1. LLM ë° ì„ë² ë”© ëª¨ë¸ ì •ì˜
#     llm = ChatUpstage()
#     embedding_model = UpstageEmbeddings(model="embedding-query")
#
#     visa_topics = [
#         "ê¸°íƒ€", "ì™¸êµ(A-1)", "ê³µë¬´(A-2)", "í˜‘ì •(A-3)", "ì‚¬ì¦ë©´ì œ(B-1)", "ê´€ê´‘í†µê³¼(B-2)",
#         "ì¼ì‹œì·¨ì¬(C-1)", "ë‹¨ê¸°ë°©ë¬¸(C-3)", "ë‹¨ê¸°ì·¨ì—…(C-4)", "ë¬¸í™”ì˜ˆìˆ (D-1)", "ìœ í•™(D-2)",
#         "ê¸°ìˆ ì—°ìˆ˜(D-3)", "ì¼ë°˜ì—°ìˆ˜(D-4)", "ì·¨ì¬(D-5)", "ì¢…êµ(D-6)", "ì£¼ì¬(D-7)",
#         "ê¸°ì—…íˆ¬ì(D-8)", "ë¬´ì—­ê²½ì˜(D-9)", "êµ¬ì§(D-10)", "êµìˆ˜(E-1)", "íšŒí™”ì§€ë„(E-2)",
#         "ì—°êµ¬(E-3)", "ê¸°ìˆ ì§€ë„(E-4)", "ì „ë¬¸ì§ì—…(E-5)", "ì˜ˆìˆ í¥í–‰(E-6)", "íŠ¹ì •í™œë™(E-7)",
#         "ê³„ì ˆê·¼ë¡œ(E-8)", "ë¹„ì „ë¬¸ì·¨ì—…(E-9)", "ì„ ì›ì·¨ì—…(E-10)", "ë°©ë¬¸ë™ê±°(F-1)", "ê±°ì£¼(F-2)",
#         "ë™ë°˜(F-3)", "ì¬ì™¸ë™í¬(F-4)", "ì˜ì£¼(F-5)", "ê²°í˜¼ì´ë¯¼(F-6)", "ê¸°íƒ€(G-1)",
#         "ê´€ê´‘ì·¨ì—…(H-1)", "ë°©ë¬¸ì·¨ì—…(H-2)", "íƒ‘í‹°ì–´ë¹„ì(D-10-T)", "íƒ‘í‹°ì–´ë¹„ì(E-7-T)",
#         "íƒ‘í‹°ì–´ë¹„ì(F-2-T)", "íƒ‘í‹°ì–´ë¹„ì(F-5-T)"
#     ]
#
#     # ì£¼ì œ ë¦¬ìŠ¤íŠ¸ (ì²´ë¥˜ ë§¤ë‰´ì–¼)
#     stay_topics = [
#         "ê¸°íƒ€", "ì™¸êµ(A-1)", "ê³µë¬´(A-2)", "í˜‘ì •(A-3)", "ì‚¬ì¦ë©´ì œ(B-1)", "ê´€ê´‘í†µê³¼(B-2)",
#         "ì¼ì‹œì·¨ì¬(C-1)", "ë‹¨ê¸°ë°©ë¬¸(C-3)", "ë‹¨ê¸°ì·¨ì—…(C-4)", "ë¬¸í™”ì˜ˆìˆ (D-1)", "ìœ í•™(D-2)",
#         "ê¸°ìˆ ì—°ìˆ˜(D-3)", "ì¼ë°˜ì—°ìˆ˜(D-4)", "ì·¨ì¬(D-5)", "ì¢…êµ(D-6)", "ì£¼ì¬(D-7)",
#         "ê¸°ì—…íˆ¬ì(D-8)", "ë¬´ì—­ê²½ì˜(D-9)", "êµ¬ì§(D-10)", "êµìˆ˜(E-1)", "íšŒí™”ì§€ë„(E-2)",
#         "ì—°êµ¬(E-3)", "ê¸°ìˆ ì§€ë„(E-4)", "ì „ë¬¸ì§ì—…(E-5)", "ì˜ˆìˆ í¥í–‰(E-6)", "íŠ¹ì •í™œë™(E-7)",
#         "ê³„ì ˆê·¼ë¡œ(E-8)", "ë¹„ì „ë¬¸ì·¨ì—…(E-9)", "ì„ ì›ì·¨ì—…(E-10)", "ë°©ë¬¸ë™ê±°(F-1)", "ê±°ì£¼(F-2)",
#         "ë™ë°˜(F-3)", "ì¬ì™¸ë™í¬(F-4)", "ì˜ì£¼(F-5)", "ê²°í˜¼ì´ë¯¼(F-6)", "ê¸°íƒ€(G-1)",
#         "ê´€ê´‘ì·¨ì—…(H-1)", "ë°©ë¬¸ì·¨ì—…(H-2)", "êµ­ë‚´ ì„±ì¥ ê¸°ë°˜ ì™¸êµ­ì¸ ì²­ì†Œë…„ ì·¨ì—….ì •ì£¼ ì²´ë¥˜ì œë„", "íƒ‘í‹°ì–´ë¹„ì(D-10-T)", "íƒ‘í‹°ì–´ë¹„ì(E-7-T)",
#         "íƒ‘í‹°ì–´ë¹„ì(F-2-T)", "íƒ‘í‹°ì–´ë¹„ì(F-5-T)"
#     ]
#
#     # 2. í”„ë¡¬í”„íŠ¸ ì •ì˜
#     prompt_template = PromptTemplate.from_template("""
#     ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì¶œì…êµ­ ê´€ë ¨ ë§¤ë‰´ì–¼ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì œê³µë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒ ë‘ ì¢…ë¥˜ì˜ ë§¤ë‰´ì–¼ ì¤‘ í•˜ë‚˜ ë˜ëŠ” ëª¨ë‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
#
#     1. ì‚¬ì¦ ë§¤ë‰´ì–¼ (Visa Manual) â€“ í•œêµ­ ì…êµ­ ì „ ë¹„ì ë°œê¸‰ì— ëŒ€í•œ ì •ë³´
#     2. ì²´ë¥˜ ë§¤ë‰´ì–¼ (Stay Manual) â€“ ì…êµ­ í›„ ì²´ë¥˜ ì—°ì¥, ì²´ë¥˜ ìê²© ë³€ê²½ ë“±ì— ëŒ€í•œ ì •ë³´
#
#     ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì˜ ì½ê³ , ì•„ë˜ ì§€ì¹¨ì— ë”°ë¼ ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ í•´ ì£¼ì„¸ìš”:
#
#     - ê° ë¹„ìì—ëŠ” ì„¸ë¶€ ìœ í˜•(sub-type)ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: D-8-1, D-8-4).
#       â†’ ê° ì„¸ë¶€ ë¹„ìë³„ ìš”ê±´ ë° ì œì¶œ ì„œë¥˜ê°€ ë‹¤ë¥´ë¯€ë¡œ, ë°˜ë“œì‹œ ì •í™•í•œ ì„¸ë¶€ ìœ í˜•ì„ êµ¬ë¶„í•˜ì—¬ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ì–¸ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
#     - ê° ë¹„ìì— ëŒ€í•œ ì œì¶œ ì„œë¥˜, ëŒ€ìƒì, ìê²©ìš”ê±´ ë“±ì€ ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•˜ëŠ”ì§€, ì•„ë‹ˆë©´ ì¼ë¶€ë§Œ ë§Œì¡±í•´ë„ ë˜ëŠ”ì§€ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•´ ì£¼ì„¸ìš”.
#     - ì»¨í…ìŠ¤íŠ¸ì— ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì£¼ì–´ì§„ ë‚´ìš©ì— ê¸°ë°˜í•´ì„œë§Œ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ì–¸ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
#
#     ---
#
#     ì§ˆë¬¸:
#     {question}
#
#     ---
#
#     ì»¨í…ìŠ¤íŠ¸:
#     {context}
#
#     ---
#
#     ì‚¬ì¦ ì£¼ì œ ë¦¬ìŠ¤íŠ¸:
#     {visa_topics}
#
#     ì²´ë¥˜ ì£¼ì œ ë¦¬ìŠ¤íŠ¸:
#     {stay_topics}
#
#     ë‹µë³€:
#     """)
#
#     chain = prompt_template | llm | StrOutputParser()
#
#     # 3. Retriever ì •ì˜
#     faiss_retriever = vectorstore.as_retriever(
#         search_type='mmr',
#         search_kwargs={"k": 20}  # ì›ë³¸ë³´ë‹¤ ì¡°ê¸ˆ ë” ë„“ê²Œ ê²€ìƒ‰
#     )
#
#     # 4. ì§ˆë¬¸ & ì •ë‹µ & ì£¼ì œ ë§¤í•‘
#     questions = [
#         "ì¬ì™¸ê³µê´€ì—ì„œ ë°©ë¬¸ì·¨ì—… ì‚¬ì¦ì„ ì‹ ì²­í•˜ëŠ” ëŒ€ìƒì€ ëˆ„êµ¬ì¸ê°€ìš”?",
#         "D-8-4 ë¹„ìë¥¼ ì·¨ë“í•˜ê³  ì‹¶ì€ë° ì œì¶œ ì„œë¥˜ í•­ëª© ì•Œë ¤ì¤˜!",
#         "D-10-2 ë¹„ìë¥¼ ì†Œì§€í•˜ê³  ìˆëŠ”ë° ì˜ì–´í‚¤ì¦ˆì¹´í˜ì—ì„œ ì¼í•´ë„ ë¼?",
#         # "D-8-4ë¥¼ ìœ„í•œ ì ìˆ˜ì œëŠ” ë­ì•¼?",
#         "ë‚´ í•œêµ­ì¸ ë‚¨ìì¹œêµ¬ì™€ ì˜¤ë˜ ì‚¬ê²¼ì§€ë§Œ ì•„ì§ ê²°í˜¼ì€ ì•ˆí–ˆì„ ë•Œ ê²°í˜¼ë¹„ìë¡œ í•œêµ­ì—ì„œ ì‚´ ìˆ˜ ìˆì–´?",
#         "D-8-4 Visaì— ëŒ€í•œ ì ìˆ˜ì œ í‘œëŠ” ì–´ë””ì„œ í™•ì¸í•  ìˆ˜ ìˆì–´?",
#     ]
#
#     ground_truths = [
#         "",
#         # "The D-8-4 Visa is a visa for A technology startup founder who has obtained an associate degree or higher domestically, or a bachelor's degree or higher abroad, or who is recommended by the head of a relevant central administrative agency and holds intellectual property rights or equivalent technological capabilities",
#         "â‘  ì‚¬ì¦ë°œê¸‰ì‹ ì²­ì„œ (ë³„ì§€ ì œ17í˜¸ ì„œì‹), ì—¬ê¶Œ, í‘œì¤€ê·œê²©ì‚¬ì§„ 1ë§¤, ìˆ˜ìˆ˜ë£Œ â‘¡ ë²•ì¸ë“±ê¸°ì‚¬í•­ì „ë¶€ì¦ëª…ì„œ ë° ì‚¬ì—…ìë“±ë¡ì¦ ì‚¬ë³¸ â‘¢ í•™ìœ„ì¦ëª…ì„œ ì‚¬ë³¸ ë˜ëŠ” ê´€ê³„ ì¤‘ì•™í–‰ì •ê¸°ê´€ì˜ ì¥ì˜ ì¶”ì²œì„œ â‘£ ì ìˆ˜ì œ í•´ë‹¹ í•­ëª©( ë° ì ìˆ˜) ì…ì¦ ì„œë¥˜",
#         "",
#         # "The corporate investment visas include the D-8-1 for foreign-invested company representatives under the Foreign Investment Promotion Act; the D-8-2 for founders of venture companies established under the Special Act on the Promotion of Venture Businesses (excluding China and certain countries); the D-8-3 qualification visa for foreigners investing in companies managed by nationals; the D-8-4 for technology startup founders with relevant academic degrees or recommendations who hold intellectual property or equivalent technological capabilities; and the D-8-4S for technology startup founders evaluated by the Startup Korea Special Visa Private Evaluation Committee and recommended by the Minister of SMEs and Startups.",
#         # "ì ìˆ˜ì œ í‘œëŠ” ì²´ë¥˜ìê²© ë§¤ë‰´ì–¼ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ì‹ì¬ì‚°ê¶Œì˜ ê²½ìš° íŠ¹í—ˆì¦ì´ë‚˜ ì‹¤ìš©ì‹ ì•ˆë“±ë¡ì¦, ë˜ëŠ” ë””ìì¸ ë“±ë¡ì¦ ì‚¬ë³¸ì„ ì œì¶œí•˜ì—¬ ì…ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹í—ˆ ë“± ì¶œì›ìëŠ” íŠ¹í—ˆì²­ì¥ ë°œí–‰ ì¶œì›ì‚¬ì‹¤ì¦ëª…ì„œë¥¼ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ë²•ë¬´ë¶€ ì¥ê´€ì´ ì§€ì •í•œ 'ê¸€ë¡œë²Œ ì°½ì—… ì´ë¯¼ ì„¼í„°'ì˜ ì¥ì´ ë°œê¸‰í•œ ì°½ì—…ì´ë¯¼ì¢…í•©ì§€ì›ì‹œìŠ¤í…œ(OASIS) í•´ë‹¹ í•­ëª© ì´ìˆ˜ ì¦ì„œ, ì…ìƒí™•ì¸ì„œ, ì„ ì •ê³µë¬¸ ë“±ìœ¼ë¡œ ì…ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°íƒ€ ì ìˆ˜ì œ í•´ë‹¹ í•­ëª© ë“±ì˜ ì…ì¦ì„œë¥˜ë“¤ì´ ìš”êµ¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."  # ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì„œ ë„£ìœ¼ì„¸ìš”,
#         "",
#         "ì²´ë¥˜ìê²© ë§¤ë‰´ì–¼ì—ì„œ D-8-4 Visaì— ëŒ€í•œ ì ìˆ˜ì œ í‘œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í‘œëŠ” í•´ë‹¹ ë¹„ìì˜ ì ìˆ˜ì œ ìš”ê±´ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì§€ì‹ì¬ì‚°ê¶Œì˜ ê²½ìš° íŠ¹í—ˆì¦, ì‹¤ìš©ì‹ ì•ˆë“±ë¡ì¦, ë””ìì¸ ë“±ë¡ì¦ ë“±ì˜ ì‚¬ë³¸ì„ ì œì¶œí•˜ì—¬ ì…ì¦í•  ìˆ˜ ìˆìœ¼ë©°, ë²•ë¬´ë¶€ ì¥ê´€ì´ ì§€ì •í•œ ê¸€ë¡œë²Œ ì°½ì—… ì´ë¯¼ ì„¼í„°ì˜ ì¥ì´ ë°œê¸‰í•œ ì¦ì„œë¡œë„ ì…ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
#     ]
#
#     # ì£¼ì œ (metadata["topic"]) ë§¤í•‘
#     # question_topic_map = {
#     #     "What is D-8-4 Visa?": "ê¸°ì—…íˆ¬ì(D-8)",
#     #     "D-8-4 ë¹„ìë¥¼ ì·¨ë“í•˜ê³  ì‹¶ì€ë° ì œì¶œ ì„œë¥˜ í•­ëª© ì•Œë ¤ì¤˜!": "ê¸°ì—…íˆ¬ì(D-8)",
#     #     "What kind of sub-visas under D-8 Visa?": "ê¸°ì—…íˆ¬ì(D-8)",
#     #     "D-8-4ë¥¼ ìœ„í•œ ì ìˆ˜ì œëŠ” ë­ì•¼?": "ê¸°ì—…íˆ¬ì(D-8)",
#     #     "D-8-4 Visaì— ëŒ€í•œ ì ìˆ˜ì œ í‘œëŠ” ì–´ë””ì„œ í™•ì¸í•  ìˆ˜ ìˆì–´?": "ê¸°ì—…íˆ¬ì(D-8)",
#     # }
#
#     # 5. QA & í‰ê°€ ë°ì´í„° êµ¬ì„±
#     faiss_data = {
#         "question": [],
#         "answer": [],
#         "contexts": [],
#         "ground_truth": [],
#     }
#
#     # # 1. í† í”½ ì¶”ë¡ ìš© ì²´ì¸ (ê¸°ì¡´ì²˜ëŸ¼ ë§Œë“¤ì–´ ë‘” chain_topic í™œìš©)
#     # for question, gt in zip(questions, ground_truths):
#     #     topic_chain = PromptTemplate.from_template("""
#     #         ë‹¤ìŒ ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ì£¼ì œë¥¼ ì•„ë˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•˜ë‚˜ ì„ íƒí•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë§ë¶™ì´ëŠ” ë§ ì—†ì´ ì£¼ì œ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”:
#
#     #         ì§ˆë¬¸: {question}
#     #         ì£¼ì œ ë¦¬ìŠ¤íŠ¸: {all_topics}
#
#     #         ì„ íƒí•œ ì£¼ì œ:""") | llm | StrOutputParser()
#
#     #     # all_topicsëŠ” visa_topics + stay_topics
#     #     all_topics = visa_topics + stay_topics
#
#     #     # 2. ì§ˆë¬¸ì— ëŒ€í•œ í† í”½ ì¶”ë¡ 
#     #     inferred_topic = topic_chain.invoke({
#     #         "question": [question for question in questions],
#     #         "all_topics": ", ".join(all_topics)
#     #     }).strip()
#     #     print(f"ğŸ” ì¶”ë¡ ëœ í† í”½: {inferred_topic}")
#     #         # topic = question_topic_map[question]
#
#     #     all_results = faiss_retriever.invoke(question)
#
#     #     # í•„í„° ì¡°ê±´ ì™„í™” (ì™„ì „ ì¼ì¹˜ ëŒ€ì‹  í¬í•¨ ì—¬ë¶€ ì‚¬ìš©)
#     #     filtered_docs = [
#     #         doc for doc in all_results
#     #         if inferred_topic in doc.metadata.get("topic", "")
#     #     ]
#
#     #     if not filtered_docs:
#     #         print(f"âš ï¸ [í† í”½ ë¶ˆì¼ì¹˜] '{question}' ì— ëŒ€í•´ topic í•„í„°ë§ëœ ë¬¸ì„œê°€ ì—†ì–´ ì „ì²´ ê²°ê³¼ ì‚¬ìš©")
#     #         context_docs = all_results
#     #     else:
#     #         context_docs = filtered_docs
#
#     #     context = [doc.page_content for doc in context_docs]
#     #     context_str = "\n\n".join(context)
#
#     #     answer = chain.invoke({"context": context_str, "question": question, "visa_topics": ", ".join(visa_topics), "stay_topics": ", ".join(stay_topics)})
#
#     #     faiss_data["question"].append(question)
#     #     faiss_data["answer"].append(answer)
#     #     faiss_data["contexts"].append(context)
#     #     faiss_data["ground_truth"].append(gt)
#     for question, gt in zip(questions, ground_truths):
#
#         topic_chain = PromptTemplate.from_template("""
#                      ë‹¤ìŒ ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ì£¼ì œë¥¼ ì•„ë˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•˜ë‚˜ ì„ íƒí•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë§ë¶™ì´ëŠ” ë§ ì—†ì´ ì£¼ì œ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”:
#
#                      ì§ˆë¬¸: {question}
#                      ì£¼ì œ ë¦¬ìŠ¤íŠ¸: {all_topics}
#
#                      ì„ íƒí•œ ì£¼ì œ:""") | llm | StrOutputParser()
#
#         # all_topicsëŠ” visa_topics + stay_topics
#         all_topics = visa_topics + stay_topics
#
#         # 1. í† í”½ ì¶”ë¡ 
#         try:
#             inferred_topic = topic_chain.invoke({
#                 "question": question,
#                 "all_topics": ", ".join(visa_topics + stay_topics)
#             }).strip()
#             print(f"{inferred_topic}")
#         except Exception as e:
#             print(f"âŒ í† í”½ ì¶”ë¡  ì‹¤íŒ¨: '{question}' / ì—ëŸ¬: {e}")
#             inferred_topic = ""
#
#         # 2. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
#         all_results = faiss_retriever.invoke(question)
#
#         # 3. í† í”½ ê¸°ë°˜ í•„í„°ë§ (ì •ìƒ ì¶”ë¡ ëœ ê²½ìš°ë§Œ)
#         if inferred_topic:
#             filtered_docs = [doc for doc in all_results if inferred_topic in doc.metadata.get("topic", "")]
#         else:
#             filtered_docs = []
#
#         # 4. ë¬¸ì„œ ì„ íƒ ì „ëµ
#         if not inferred_topic:
#             print(f"âš ï¸ [í† í”½ ì¶”ë¡  ì‹¤íŒ¨] '{question}' â†’ ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
#             context_docs = all_results
#         elif not filtered_docs:
#             print(f"âš ï¸ [í† í”½ ë¶ˆì¼ì¹˜] '{question}' â†’ ì¶”ë¡ ëœ í† í”½: {inferred_topic}, í•„í„°ë§ëœ ë¬¸ì„œ ì—†ìŒ â†’ ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
#             context_docs = all_results
#         else:
#             print(f"ğŸ” ì¶”ë¡ ëœ í† í”½: {inferred_topic}")
#             context_docs = filtered_docs
#
#         # 5. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
#         context = [doc.page_content for doc in context_docs]
#         context_str = "\n\n".join(context)
#
#         # 6. LLM í˜¸ì¶œ (í•„ìˆ˜ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ í¬í•¨)
#         answer = chain.invoke({
#             "context": context_str,
#             "question": question,
#             "visa_topics": ", ".join(visa_topics),
#             "stay_topics": ", ".join(stay_topics)
#         })
#
#         # 7. ì €ì¥
#         faiss_data["question"].append(question)
#         faiss_data["answer"].append(answer)
#         faiss_data["contexts"].append(context)
#         faiss_data["ground_truth"].append(gt)
#
#     # í•„í„° ì—†ì´ ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©
#     # for question, gt in zip(questions, ground_truths):
#     #     all_results = faiss_retriever.invoke(question)
#
#     #
#     #     context = [doc.page_content for doc in all_results]
#     #     context_str = "\n\n".join(context)
#
#     #     answer = chain.invoke({"context": context_str, "question": question})
#
#     #     faiss_data["question"].append(question)
#     #     faiss_data["answer"].append(answer)
#     #     faiss_data["contexts"].append(context)
#     #     faiss_data["ground_truth"].append(gt)
#
#     # 6. RAGAS í‰ê°€
#     faiss_dataset = Dataset.from_dict(faiss_data)
#     faiss_score = evaluate(
#         faiss_dataset,
#         metrics=[context_precision, context_recall],
#         llm=llm,
#         embeddings=embedding_model,
#     )
#     print("ğŸ“Š FAISS Evaluation Score:", faiss_score)
#
#     # 7. ì¶œë ¥
#     for idx, (q, a) in enumerate(zip(faiss_data["question"], faiss_data["answer"])):
#         print(f"{idx + 1} Q: {q}\nA: {a}\n{'-' * 40}")
