
# from pdfminer.high_level import extract_text
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import json
import time
import tiktoken
from tqdm import tqdm
from langchain_upstage import ChatUpstage
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain_upstage import UpstageEmbeddings
from ragas import evaluate
from langchain.memory import ConversationBufferMemory
from rapidfuzz import fuzz
from datasets import Dataset
from langchain.embeddings import OpenAIEmbeddings
from ragas.metrics import context_precision, context_recall, faithfulness


UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")



# ---------------------------------------

def load_vectorstore(UPSTAGE_API_KEY):
    # 1. ê°™ì€ ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë‹¤ì‹œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤ (ì´ê²Œ ì¤‘ìš”!)
    embedding_model = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model="solar-embedding-1-large-passage")

    # 2. FAISS ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ì§€ì • (ì˜ˆ: 'faiss_store')
    vectorstore = FAISS.load_local("faiss_vector_store", embedding_model, allow_dangerous_deserialization=True)
    return vectorstore


def get_answer(vectorstore, question, lang, memory):
    llm = ChatUpstage(
    model="solar-1-mini-chat",  # ë˜ëŠ” "solar-1-mini-32k" ë“± ì‚¬ìš© ê°€ëŠ¥
    temperature=0.3
    )
    language = lang

    # 2. í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompt_template = PromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì¶œì…êµ­ ê´€ë ¨ ë§¤ë‰´ì–¼ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì´ì í†µì—­ê°€ì…ë‹ˆë‹¤. 

        
                                                   
        ---
        ë‹¤ìŒì€ ì°¸ê³ í•  ì •ë³´ì…ë‹ˆë‹¤. 

        ì»¨í…ìŠ¤íŠ¸:
        {context}
                                                   
        ì´ì „ ëŒ€í™” ê¸°ë¡: 
        {history}

        ì§ˆë¬¸:
        {question}

        ---
                                                   
        ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì˜ ì½ê³ , ì•„ë˜ ì§€ì¹¨ì— ë”°ë¼ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ì–¸ì–´ì¸ {language}ìœ¼ë¡œ ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ í•´ ì£¼ì„¸ìš”. ì ì ˆí•œ ì£¼ì œì™€ ë‚´ìš©, ì´ëª¨ì§€ë¡œ êµ¬ì„±í•˜ì—¬ ì½ê¸° ì‰½ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.:
        - ë°˜ë“œì‹œ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ì–¸ì–´ì¸ {language}ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”!!!
        - ê° ë¹„ìì—ëŠ” ì„¸ë¶€ ìœ í˜•(sub-type)ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: D-8-1, D-8-4).  
          â†’ ê° ì„¸ë¶€ ë¹„ìë³„ ìš”ê±´ ë° ì œì¶œ ì„œë¥˜ê°€ ë‹¤ë¥´ë¯€ë¡œ, ë°˜ë“œì‹œ ì •í™•í•œ ì„¸ë¶€ ìœ í˜•ì„ êµ¬ë¶„í•˜ì—¬ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ì–¸ì–´ì¸ {language}ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        - ê° ë¹„ìì— ëŒ€í•œ ì œì¶œ ì„œë¥˜, ëŒ€ìƒì, ìê²©ìš”ê±´ ë“±ì€ ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•˜ëŠ”ì§€, ì•„ë‹ˆë©´ ì¼ë¶€ë§Œ ë§Œì¡±í•´ë„ ë˜ëŠ”ì§€ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        - ì ìˆ˜ì œì—ë„ ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. ë¹„ìì— ë”°ë¼ í•´ë‹¹í•˜ëŠ” ì ìˆ˜ì œê°€ ë‹¬ë¼ì§€ë‹ˆ ìœ ì˜í•´ì„œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        - ì»¨í…ìŠ¤íŠ¸ì— ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì£¼ì–´ì§„ ì •ë³´ë§Œ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”..
        - ì´ì „ ëŒ€í™”ì— í¬í•¨ëœ ì§ˆë¬¸ì´ë¼ë©´ ê°„ê²°í•˜ê²Œ ë‹¤ì‹œ ì„¤ëª…í•˜ê³ , ìƒˆ ì •ë³´ê°€ ìˆë‹¤ë©´ ê·¸ ìœ„ì£¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        - ë‹µë³€ì—ëŠ” ì§ˆë¬¸ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì‹œê³ , ë°”ë¡œ ë‹µë³€ë¶€í„° ì‹œì‘í•˜ì„¸ìš”.
                                                   
        --- 

        ë‹µë³€:
        """)

    chain = prompt_template | llm | StrOutputParser()

    # 3. Retriever ì •ì˜
    retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 3, "fetch_k": 20, "lambda_mult": 0.9},
    )


    history_str = memory.load_memory_variables({}).get("history", "")

    #  íˆìŠ¤í† ë¦¬ë¥¼ ê²€ìƒ‰ ë‹¨ê³„ì—ì„œ ë°˜ì˜
    # search_query = (history_str + "\n" if history_str else "") + question
    docs = retriever.invoke(question)

    # contextëŠ” docsì—ì„œ ì¶”ì¶œ
    context = "\n\n".join([doc.page_content for doc in docs])
    # context_str = (history_str + "\n\n" if history_str else "") + context

    # 6. LLM í˜¸ì¶œ (í•„ìˆ˜ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ í¬í•¨)
    answer = chain.invoke({
        "context": context,
        "question": question,
        "language": language,
        "history" : history_str
    })
    
    memory.save_context({"input": question}, {"output": answer})

    return answer


# pdf_paths = ["stay.pdf", "visa.pdf"]  # ì—¬ê¸°ì— ë‘ PDF ê²½ë¡œ ì…ë ¥

# all_text_blocks = []
# all_tables = []

# for pdf_path in pdf_paths:
#     with pdfplumber.open(pdf_path) as pdf:
#         for i, page in enumerate(pdf.pages):
#             text = page.extract_text()
#             tables = page.extract_tables()

#             # í…ìŠ¤íŠ¸ ë¬¸ë‹¨ êµ¬ì¡°í™”
#             if text:
#                 paragraphs = text.split('\n\n')  # ì¤„ë°”ê¿ˆ ê¸°ì¤€ ë‚˜ëˆ„ê¸°
#                 for para in paragraphs:
#                     all_text_blocks.append({
#                         "file": os.path.basename(pdf_path),
#                         "page": i + 1,
#                         "type": "text",
#                         "content": para.strip()
#                     })

#             # í‘œ êµ¬ì¡°í™”
#             for table in tables:
#                 if table and len(table) > 1:
#                     headers = table[0]
#                     rows = table[1:]
#                     all_tables.append({
#                         "file": os.path.basename(pdf_path),
#                         "page": i + 1,
#                         "type": "table",
#                         "headers": headers,
#                         "rows": rows
#                     })


# # í™•ì¸
# # print(all_text_blocks[:1000])  # ì²˜ìŒ ì¼ë¶€ í…ìŠ¤íŠ¸ ì¶œë ¥
# # print(all_tables[1])


# """### ë¬¸ë§¥ ë³„ë¡œ ì²­í¬í•˜ê¸°
# - í‰ê·  í† í° ìˆ˜: 2000ê°œ
# - ìµœëŒ€ í† í° ìˆ˜ : ì•½ 4800ê°œ
# - ìµœì†Œ í† í° ìˆ˜: ì•½ 30ê°œ
# """

# # ë¬¸ë§¥ ë³„ë¡œ ì²­í¬í•˜ê¸°
# def table_to_markdown(headers, rows):
#     # None â†’ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
#     headers = [str(h) if h is not None else "" for h in headers]
#     header_line = "| " + " | ".join(headers) + " |"
#     separator = "| " + " | ".join(["---"] * len(headers)) + " |"

#     row_lines = []
#     for row in rows:
#         cleaned_row = [str(cell) if cell is not None else "" for cell in row]
#         row_lines.append("| " + " | ".join(cleaned_row) + " |")

#     return "\n".join([header_line, separator] + row_lines)


# def build_semantic_chunks_by_proximity(all_chunks, window=1):
#     semantic_chunks = []

#     # ì •ë ¬
#     all_chunks.sort(key=lambda x: (x['file'], x['page']))

#     for i, chunk in enumerate(all_chunks):
#         if chunk["type"] == "table":
#             # ì£¼ë³€ í…ìŠ¤íŠ¸ ì°¾ê¸°
#             related_texts = []
#             for offset in range(-window, window + 1):
#                 j = i + offset
#                 if 0 <= j < len(all_chunks) and all_chunks[j]["type"] == "text":
#                     related_texts.append(all_chunks[j]["content"])

#             # í‘œ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
#             table_md = table_to_markdown(chunk["headers"], chunk["rows"])

#             combined = "\n\n".join(related_texts + [table_md])

#             # í† í° ìˆ˜ ì¸¡ì •
#             # token_count = count_tokens(combined)

#             semantic_chunks.append({
#                 "content": combined,
#                 "metadata": {
#                     "file": chunk["file"],
#                     "page": chunk["page"]
#                 }
#             })

#     return semantic_chunks

# semantic_chunks = build_semantic_chunks_by_proximity(all_text_blocks + all_tables, window=1)

# # ì˜ˆì‹œ ì¶œë ¥
# for i, chunk in enumerate(semantic_chunks[:3]):
#     print(f"\n--- Chunk {i+1} ---\n")
#     print(chunk["content"][:1000])  # ë‚´ìš© ì¼ë¶€ë§Œ ì¶œë ¥

# import json

# # 1. Tokenizer ì„¤ì • (GPT-3.5 ê¸°ì¤€)
# encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

# def count_tokens(text):
#     return len(encoding.encode(text))

# def split_by_token_window(text, size=500, overlap=100):
#     tokens = encoding.encode(text)
#     chunks = []

#     start = 0
#     while start < len(tokens):
#         end = start + size
#         chunk_tokens = tokens[start:end]
#         chunk_text = encoding.decode(chunk_tokens)
#         chunks.append(chunk_text)

#         start += size - overlap

#     return chunks

# final_chunks = []

# for chunk in semantic_chunks:
#     split_contents = split_by_token_window(chunk["content"], size=500, overlap=100)
#     for split_text in split_contents:
#         final_chunks.append({
#             "content": split_text,
#             "metadata": chunk["metadata"]
#         })

# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.schema import Document
# # all_text_blocks.json ë¶ˆëŸ¬ì˜¤ê¸°
# with open('all_text_blocks.json', 'r', encoding='utf-8') as f:
#     raw_blocks = json.load(f)

# # all_tables.json ë¶ˆëŸ¬ì˜¤ê¸°
# # with open('all_tables.json', 'r', encoding='utf-8') as f:
# #     all_tables = json.load(f)

# all_text_blocks = [
#     Document(page_content=block["content"], metadata={"file": block["file"], "page": block["page"]})
#     for block in raw_blocks
# ]

# text_splitter = RecursiveCharacterTextSplitter(
#     chunk_size=1000,
#     chunk_overlap=100
# )

# all_text_blocks
# splits = text_splitter.split_documents(all_text_blocks)

# print("splits", len(splits))
# print(splits[:3])

# """### chunk ê°œìˆ˜, í‰ê·  í† í° í™•ì¸"""

# # ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ tokenizer ì„¤ì •
# encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

# def count_tokens(text):
#     return len(encoding.encode(text))

# # ì „ì²´ í† í° ìˆ˜ì™€ í†µê³„ êµ¬í•˜ê¸°
# token_counts = [count_tokens(chunk['content']) for chunk in final_chunks]

# avg_tokens = sum(token_counts) / len(token_counts)
# max_tokens = max(token_counts)
# min_tokens = min(token_counts)

# print(f"âœ… ì´ chunk ìˆ˜: {len(final_chunks)}")
# print(f"ğŸ“Š í‰ê·  í† í° ìˆ˜: {avg_tokens:.2f}")
# print(f"ğŸ”º ìµœëŒ€ í† í° ìˆ˜: {max_tokens}")
# print(f"ğŸ”» ìµœì†Œ í† í° ìˆ˜: {min_tokens}")

# """## ì„ë² ë”©"""

# # í…ìŠ¤íŠ¸ + ë¬¸ì„œ ì²­í¬ ì‹œ ë­ì²´ì¸ documentë¡œ ë³€í™˜í•´ì£¼ëŠ” ì½”ë“œ

# splits = [
#     Document(page_content=chunk["content"], metadata=chunk["metadata"])
#     for chunk in final_chunks
# ]

# # ì„ë² ë”©
# embedding_model = UpstageEmbeddings(model="solar-embedding-1-large-passage")

# vectors = []
# documents = []

# for i, doc in enumerate(splits):
#     try:
#         vectorstore = FAISS.from_documents([doc], embedding=embedding_model)
#         if i == 0:
#             main_vectorstore = vectorstore
#         else:
#             main_vectorstore.merge_from(vectorstore)
#         time.sleep(1)  # ìš”ì²­ ì†ë„ ì œí•œ ë°©ì§€
#     except Exception as e:
#         print(f"â— Doc {i} ì„ë² ë”© ì‹¤íŒ¨: {e}")



# # ì²« ë²ˆì§¸ ë¬¸ì„œ í™•ì¸
# doc = vectorstore.docstore.search("0")  # ë˜ëŠ” list(vectorstore.docstore._dict.keys())[0]
# print("ğŸ‘‰ ìƒ˜í”Œ ë¬¸ì„œ:", doc)


# from langchain_core.prompts import ChatPromptTemplate

# query ="D-8 ë¹„ìì— ëŒ€í•´ ì•Œë ¤ì¤˜"

# # 4. Dense Retriever ìƒì„±
# retriever = vectorstore.as_retriever(
#     search_type="mmr",
#     search_kwargs={"k": 3},
# )

# # 5. ChatPromptTemplate ì •ì˜
# result_docs = retriever.invoke(query)

# # 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
# prompt = ChatPromptTemplate.from_messages(
#     [
#         (
#             "system",
#             # system context
#             """
#             ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¹„ì/ì²´ë¥˜ ìê²© ê´€ë ¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µí•´ì£¼ì„¸ìš”.
#             ë‹µì„ ëª¨ë¥´ê² ìœ¼ë©´ ëŒ€ë‹µí•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
#             ë¬¸ì„œ: {context}
#             """,
#         ),
#         ("human", "{input}"),
#     ]
# )


# # 2. LLM ëª¨ë¸ ì„¤ì • (Upstage ì˜ˆì‹œ)
# llm = ChatUpstage(
#     model="solar-1-mini-chat",  # ë˜ëŠ” "solar-1-mini-32k" ë“± ì‚¬ìš© ê°€ëŠ¥
#     temperature=0.3
# )

# # 3. í”„ë¡¬í”„íŠ¸ì— ê°’ ì‚½ì… í›„ ì‹¤í–‰
# chain = prompt | llm | StrOutputParser()

# docs = retriever.invoke(query)

# # contextëŠ” docsì—ì„œ ì¶”ì¶œ
# context_text = "\n\n".join([doc.page_content for doc in docs])
# response = chain.invoke({"input": query, "context": context_text})
# # response = chain.invoke({"question": query})

# print(response)

# """RAGAS í‰ê°€"""

# data_sets = [
#     {
#         "question": "D-8-4 ë¹„ìë€ ë¬´ì—‡ì¸ê°€ìš”?",
#         "answer": "D-8-4 ë¹„ìëŠ” ê¸°ìˆ  ì°½ì—…ìë¥¼ ìœ„í•œ ë¹„ìë¡œ, êµ­ë‚´ì™¸ í•™ìœ„ ë³´ìœ ìë‚˜ ì •ë¶€ ì¶”ì²œì„ ë°›ì€ ì‚¬ëŒì´ ì‹ ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
#         "contexts": [
#             "D-8-4 ë¹„ìëŠ” êµ­ë‚´ì—ì„œ ì „ë¬¸í•™ì‚¬ ì´ìƒì˜ í•™ìœ„ë¥¼ ì·¨ë“í–ˆê±°ë‚˜ êµ­ì™¸ì—ì„œ í•™ì‚¬ ì´ìƒì˜ í•™ìœ„ë¥¼ ì·¨ë“í–ˆê±°ë‚˜, ê´€ê³„ ì¤‘ì•™í–‰ì •ê¸°ê´€ì˜ ì¥ì˜ ì¶”ì²œì„ ë°›ì€ ìë¡œì„œ ì§€ì‹ì¬ì‚°ê¶Œ ë˜ëŠ” ì´ì— ì¤€í•˜ëŠ” ê¸°ìˆ ë ¥ì„ ë³´ìœ í•œ ê¸°ìˆ  ì°½ì—…ìë¥¼ ìœ„í•œ ë¹„ìì…ë‹ˆë‹¤."
#         ],
#         "ground_truth": "D-8-4 ë¹„ìëŠ” êµ­ë‚´ì—ì„œ ì „ë¬¸í•™ì‚¬ ì´ìƒì˜ í•™ìœ„ë¥¼ ì·¨ë“í–ˆê±°ë‚˜ êµ­ì™¸ì—ì„œ í•™ì‚¬ ì´ìƒì˜ í•™ìœ„ë¥¼ ì·¨ë“í–ˆê±°ë‚˜, ê´€ê³„ ì¤‘ì•™í–‰ì •ê¸°ê´€ì˜ ì¥ì˜ ì¶”ì²œì„ ë°›ì€ ìë¡œì„œ ì§€ì‹ì¬ì‚°ê¶Œ ë˜ëŠ” ì´ì— ì¤€í•˜ëŠ” ê¸°ìˆ ë ¥ì„ ë³´ìœ í•œ ê¸°ìˆ  ì°½ì—…ìë¥¼ ìœ„í•œ ë¹„ìì…ë‹ˆë‹¤."
#     },
#     {
#         "question": "D-8-4 ë¹„ìì— í•„ìš”í•œ ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
#         "answer": "ì‚¬ì¦ì‹ ì²­ì„œ, ì—¬ê¶Œ, ì‚¬ì§„, í•™ìœ„ì¦ëª…ì„œ, ì§€ì‹ì¬ì‚°ê¶Œ ê´€ë ¨ ì„œë¥˜, OASIS ìˆ˜ë£Œì¦ ë“±ì´ í•„ìš”í•©ë‹ˆë‹¤.",
#         "contexts": [
#             "D-8-4 ë¹„ìì— í•„ìš”í•œ ì„œë¥˜ë¡œëŠ” ì‚¬ì¦ë°œê¸‰ì¸ì •ì‹ ì²­ì„œ(ë³„ì§€ ì œ17í˜¸ ì„œì‹), ì—¬ê¶Œ, ê·œê²© ì‚¬ì§„, ìˆ˜ìˆ˜ë£Œê°€ í¬í•¨ë˜ë©°, ë²•ì¸ì„¤ë¦½ì‹ ê³ í™•ì¸ì„œì™€ ì‚¬ì—…ìë“±ë¡ì¦ ì‚¬ë³¸, í•™ìœ„ì¦ëª…ì„œ ë˜ëŠ” ê´€ê³„ ì¤‘ì•™í–‰ì •ê¸°ê´€ì¥ì˜ ì¶”ì²œì„œ ì‚¬ë³¸ì´ í¬í•¨ë©ë‹ˆë‹¤. ë˜í•œ, í¬ì¸íŠ¸ì œ ê´€ë ¨ í•­ëª©(ë° ì ìˆ˜)ì„ ì¦ëª…í•˜ëŠ” ì„œë¥˜ê°€ í•„ìš”í•˜ë©°, ì§€ì‹ì¬ì‚°ê¶Œ ë³´ìœ ìëŠ” íŠ¹í—ˆì¦, ì‹¤ìš©ì‹ ì•ˆë“±ë¡ì¦, ë””ìì¸ë“±ë¡ì¦ ì‚¬ë³¸ì„ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ì°¸ê³ ë¡œ, íŠ¹í—ˆì²­ì˜ 'íŠ¹í—ˆì •ë³´ë„· í‚¤í”„ë¦¬ìŠ¤'(www.kipris.or.kr)ì—ì„œ ì§€ì‹ì¬ì‚°ê¶Œ ë³´ìœ  ì—¬ë¶€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹í—ˆ ì¶œì›ìëŠ” íŠ¹í—ˆì²­ì¥ì´ ë°œê¸‰í•œ ì¶œì›ì‚¬ì‹¤ì¦ëª…ì„œë¥¼ ì œì¶œí•´ì•¼ í•˜ë©°, ë²•ë¬´ë¶€ ì¥ê´€ì´ ì§€ì •í•œ ì´ë¯¼ì°½ì—…ì§€ì› í”„ë¡œê·¸ë¨(OASIS) ì´ìˆ˜(ìˆ˜ë£Œ, ì¡¸ì—…) ì¦ëª…ì„œ, ìˆ˜ìƒ ì¦ì„œ, ê³µê³ ë¬¸ ë“± ê´€ë ¨ ì„œë¥˜ë„ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ë²•ë¬´ë¶€ì˜ ì¶œì…êµ­ ì •ì±…ì— ë”°ë¥´ë©´, OASIS ìˆ˜ë£Œì¦ì˜ ìœ íš¨ê¸°ê°„ì€ ë°œê¸‰ì¼ë¡œë¶€í„° 2ë…„ì…ë‹ˆë‹¤. í¬ì¸íŠ¸ì œ í•­ëª©ì„ ì…ì¦í•˜ëŠ” ê¸°íƒ€ ì„œë¥˜ë„ í•¨ê»˜ ì œì¶œí•´ì•¼ í•˜ë©°, ì´ˆì²­ ëª©ì , ì´ˆì²­ ì§„ì •ì„±, ì´ˆì²­ì ë° í”¼ì´ˆì²­ìì˜ ìê²© í™•ì¸ ë“±ì„ ìœ„í•´ ì™¸êµ­ ê³µê³µê¸°ê´€ì¥ì€ ì²¨ë¶€ì„œë¥˜ë¥¼ ì¼ë¶€ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
#         ],
#         "ground_truth": "D-8-4 ë¹„ìì— í•„ìš”í•œ ì„œë¥˜ë¡œëŠ” ì‚¬ì¦ë°œê¸‰ì¸ì •ì‹ ì²­ì„œ(ë³„ì§€ ì œ17í˜¸ ì„œì‹), ì—¬ê¶Œ, ê·œê²© ì‚¬ì§„, ìˆ˜ìˆ˜ë£Œê°€ í¬í•¨ë˜ë©°, ë²•ì¸ì„¤ë¦½ì‹ ê³ í™•ì¸ì„œì™€ ì‚¬ì—…ìë“±ë¡ì¦ ì‚¬ë³¸, í•™ìœ„ì¦ëª…ì„œ ë˜ëŠ” ê´€ê³„ ì¤‘ì•™í–‰ì •ê¸°ê´€ì¥ì˜ ì¶”ì²œì„œ ì‚¬ë³¸ì´ í¬í•¨ë©ë‹ˆë‹¤. ë˜í•œ, í¬ì¸íŠ¸ì œ ê´€ë ¨ í•­ëª©(ë° ì ìˆ˜)ì„ ì¦ëª…í•˜ëŠ” ì„œë¥˜ê°€ í•„ìš”í•˜ë©°, ì§€ì‹ì¬ì‚°ê¶Œ ë³´ìœ ìëŠ” íŠ¹í—ˆì¦, ì‹¤ìš©ì‹ ì•ˆë“±ë¡ì¦, ë””ìì¸ë“±ë¡ì¦ ì‚¬ë³¸ì„ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ì°¸ê³ ë¡œ, íŠ¹í—ˆì²­ì˜ 'íŠ¹í—ˆì •ë³´ë„· í‚¤í”„ë¦¬ìŠ¤'(www.kipris.or.kr)ì—ì„œ ì§€ì‹ì¬ì‚°ê¶Œ ë³´ìœ  ì—¬ë¶€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹í—ˆ ì¶œì›ìëŠ” íŠ¹í—ˆì²­ì¥ì´ ë°œê¸‰í•œ ì¶œì›ì‚¬ì‹¤ì¦ëª…ì„œë¥¼ ì œì¶œí•´ì•¼ í•˜ë©°, ë²•ë¬´ë¶€ ì¥ê´€ì´ ì§€ì •í•œ ì´ë¯¼ì°½ì—…ì§€ì› í”„ë¡œê·¸ë¨(OASIS) ì´ìˆ˜(ìˆ˜ë£Œ, ì¡¸ì—…) ì¦ëª…ì„œ, ìˆ˜ìƒ ì¦ì„œ, ê³µê³ ë¬¸ ë“± ê´€ë ¨ ì„œë¥˜ë„ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ë²•ë¬´ë¶€ì˜ ì¶œì…êµ­ ì •ì±…ì— ë”°ë¥´ë©´, OASIS ìˆ˜ë£Œì¦ì˜ ìœ íš¨ê¸°ê°„ì€ ë°œê¸‰ì¼ë¡œë¶€í„° 2ë…„ì…ë‹ˆë‹¤. í¬ì¸íŠ¸ì œ í•­ëª©ì„ ì…ì¦í•˜ëŠ” ê¸°íƒ€ ì„œë¥˜ë„ í•¨ê»˜ ì œì¶œí•´ì•¼ í•˜ë©°, ì´ˆì²­ ëª©ì , ì´ˆì²­ ì§„ì •ì„±, ì´ˆì²­ì ë° í”¼ì´ˆì²­ìì˜ ìê²© í™•ì¸ ë“±ì„ ìœ„í•´ ì™¸êµ­ ê³µê³µê¸°ê´€ì¥ì€ ì²¨ë¶€ì„œë¥˜ë¥¼ ì¼ë¶€ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
#     },
#     {
#         "question": "D-8 ë¹„ìì—ëŠ” ì–´ë–¤ í•˜ìœ„ ë¹„ìê°€ ìˆë‚˜ìš”?",
#         "answer": "D-8 ë¹„ìëŠ” ì´ 5ê°œ í•˜ìœ„ ìœ í˜•ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. D-8-1, D-8-2, D-8-3, D-8-4, D-8-4Sì…ë‹ˆë‹¤.",
#         "contexts": [
#             "ê¸°ì—…íˆ¬ì ë¹„ìì—ëŠ” ì™¸êµ­ì¸íˆ¬ìì´‰ì§„ë²•ì— ë”°ë¥¸ ì™¸êµ­ì¸ íˆ¬ìê¸°ì—…ì˜ ëŒ€í‘œì ëŒ€ìƒ D-8-1 ë¹„ì, ë²¤ì²˜ê¸°ì—…ìœ¡ì„±ì—ê´€í•œíŠ¹ë³„ì¡°ì¹˜ë²•ì— ë”°ë¼ ì„¤ë¦½ëœ ë²¤ì²˜ê¸°ì—…ì˜ ì°½ì—…ì ëŒ€ìƒ D-8-2 ë¹„ì(ì¤‘êµ­ ë“± íŠ¹ì •êµ­ê°€ ì œì™¸), ë‚´êµ­ì¸ì´ ê²½ì˜í•˜ëŠ” ê¸°ì—…ì— íˆ¬ìí•˜ëŠ” ì™¸êµ­ì¸ì„ ìœ„í•œ D-8-3 ë¹„ì, ê´€ë ¨ í•™ìœ„ë¥¼ ë³´ìœ í•˜ê±°ë‚˜ ì¤‘ì•™í–‰ì •ê¸°ê´€ì˜ ì¶”ì²œì„ ë°›ì€ ê¸°ìˆ ì°½ì—…ìë¥¼ ìœ„í•œ D-8-4 ë¹„ì, ìŠ¤íƒ€íŠ¸ì—… ì½”ë¦¬ì•„ íŠ¹ë³„ë¹„ì ë¯¼ê°„í‰ê°€ìœ„ì›íšŒì˜ í‰ê°€ì™€ ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ ì¥ê´€ì˜ ì¶”ì²œì„ ë°›ì€ ê¸°ìˆ ì°½ì—…ìë¥¼ ìœ„í•œ D-8-4S ë¹„ìê°€ í¬í•¨ë©ë‹ˆë‹¤."
#         ],
#         "ground_truth": "ê¸°ì—…íˆ¬ì ë¹„ìì—ëŠ” ì™¸êµ­ì¸íˆ¬ìì´‰ì§„ë²•ì— ë”°ë¥¸ ì™¸êµ­ì¸ íˆ¬ìê¸°ì—…ì˜ ëŒ€í‘œì ëŒ€ìƒ D-8-1 ë¹„ì, ë²¤ì²˜ê¸°ì—…ìœ¡ì„±ì—ê´€í•œíŠ¹ë³„ì¡°ì¹˜ë²•ì— ë”°ë¼ ì„¤ë¦½ëœ ë²¤ì²˜ê¸°ì—…ì˜ ì°½ì—…ì ëŒ€ìƒ D-8-2 ë¹„ì(ì¤‘êµ­ ë“± íŠ¹ì •êµ­ê°€ ì œì™¸), ë‚´êµ­ì¸ì´ ê²½ì˜í•˜ëŠ” ê¸°ì—…ì— íˆ¬ìí•˜ëŠ” ì™¸êµ­ì¸ì„ ìœ„í•œ D-8-3 ë¹„ì, ê´€ë ¨ í•™ìœ„ë¥¼ ë³´ìœ í•˜ê±°ë‚˜ ì¤‘ì•™í–‰ì •ê¸°ê´€ì˜ ì¶”ì²œì„ ë°›ì€ ê¸°ìˆ ì°½ì—…ìë¥¼ ìœ„í•œ D-8-4 ë¹„ì, ìŠ¤íƒ€íŠ¸ì—… ì½”ë¦¬ì•„ íŠ¹ë³„ë¹„ì ë¯¼ê°„í‰ê°€ìœ„ì›íšŒì˜ í‰ê°€ì™€ ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ ì¥ê´€ì˜ ì¶”ì²œì„ ë°›ì€ ê¸°ìˆ ì°½ì—…ìë¥¼ ìœ„í•œ D-8-4S ë¹„ìê°€ í¬í•¨ë©ë‹ˆë‹¤."
#     }
# ]


# # # 1. ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
# # questions = [
# #     "D-8-4 ë¹„ìëŠ” ëˆ„êµ¬ë¥¼ ìœ„í•œ ë¹„ìì¸ê°€ìš”?",
# #     "D-8-4 ë¹„ìì— ì œì¶œí•´ì•¼ í•˜ëŠ” ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
# #     "D-8 ë¹„ìì˜ í•˜ìœ„ ìœ í˜•ì—ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?"
# # ]

# # 2. ë¹ˆ ë”•ì…”ë„ˆë¦¬
# dense_data = {
#    "question": [],
#     "answer": [],
#     "contexts": [],
#     "ground_truth": [],
# }

# retriever = vectorstore.as_retriever(
#     search_type="mmr",
#     # lambda_mult (1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìœ ì‚¬ë„, 0.1ì€ ë‹¤ì–‘ì„± ì¤‘ì‹¬)
#     search_kwargs={"k": 3, "fetch_k": 20, "lambda_mult": 0.9}
# )

# # 3. context ì±„ìš°ëŠ” í•¨ìˆ˜
# def fill_data(data, question, answer, retriever, ground_truth):
#     results = retriever.invoke(question)
#     context = [doc.page_content for doc in results]

#     data["question"].append(question)
#     data["answer"].append(answer)           # LLM ë˜ëŠ” ìˆ˜ë™ ì…ë ¥
#     data["contexts"].append(context)
#     data["ground_truth"].append(ground_truth)          # ê¸°ì¤€ ì •ë‹µ ì…ë ¥

# # 4. fill_dataë¡œ ë°ì´í„° ì±„ìš°ê¸°
# for data in enumerate(data_sets):
#     fill_data(dense_data, data["question"], data["answer"], retriever, data["ground_truth"])

# # 5. Dataset ìƒì„±
# dense_dataset = Dataset.from_dict(dense_data)

# # 6. í‰ê°€
# result = evaluate(
#     dataset=dense_dataset,
#     metrics=[context_precision, context_recall, faithfulness],
#     llm=llm,                # faithfulness ë“± LLM ê¸°ë°˜ ë©”íŠ¸ë¦­ì— í•„ìš”
#     embeddings= UpstageEmbeddings(model="embedding-query")
#     # retriever = retriever
# )

# print(result)
